complete_cases <- subset(complete_cases, nobs > threshold )
complete_cases <- subset(complete_cases, nobs > 150 )
View(complete_cases)
source("http://d396qusza40orc.cloudfront.net/rprog%2Fscripts%2Fsubmitscript1.R")
submit()
6
submit()
submit()
submit()
submit()
submit()
submit()
submit()
source("corr.R")
source("complete.R")
cr <- corr("specdata", 150)
head(cr)
source("corr.R")
source("complete.R")
cr <- corr("specdata", 150)
head(cr)
summary(cr)
cr <- corr("specdata", 400)
head(cr)
summary(cr)
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata", 5000)
summary(cr)
print(complete_cases[["id"]])
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata", 500)
summary(cr)
source("corr.R")
source("complete.R")
source("corr.R")
source("complete.R")
cr <- corr("specdata", 5000)
summary(cr)
source("corr.R")
source("complete.R")
cr <- corr("specdata", 5000)
summary(cr)
cr <- corr("specdata", 150)
head(cr)
cr <- corr("specdata", 5000)
summary(cr)
length(cr)
cr <- corr("specdata")
summary(cr)
length(cr)
submit()
submit()
submit()
library(swirl)
swirl()
?sample
sample(1:6, 4, replace = TRUE)
sample(1:6, 4, replace = TRUE)
sample(1:20, 10,)
LETTERS
sample(LETTERS)
flips <- sample(c(0,1), 100, replace = TRUE, prob = c(0.3,0.7))
flips
sum(flips)
?rbinom
rbinom(1, size = 100, prob = 0.7)
flips2 <- rbinom(100, size = 1, prob = 0.7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(10,100,25)
?rpois
rpois(5, 10)
replicate(100, rpois(5, 10))
my_pois <- replicate(100, rpois(5, 10))
my_pois
cm <- colMeans(my_pois)
hist(cm)
data(cars)
?cars
head(cars)
plot(cars)
?plot
plot(x = cars$speed, y = cars$dist)
plot(y = cars$speed, x = cars$dist)
plot(x = cars$speed, y = cars$dist, xlab = "Speed")
plot(x = cars$speed, y = cars$dist, xlab = "Speed", ylab = "Stopping Distance")
plot(x = cars$speed, y = cars$dist, ylab = "Stopping Distance")
plot(x = cars$speed, y = cars$dist, xlab = "Speed", ylab = "Stopping Distance")
plot(cars, main = "My Plot")
plot(cars, main2 = "My Plot Subtitle")
plot(cars, sub = "My Plot Subtitle")
plot(cars, col = 2)
plot(cars, xlim = c(10,15))
plot(cars, pch = 2)
data(mtcars)
?boxplot
boxplot(format.POSIXct)
boxplot(formula = mpg ~ cyl,  data = mtcars )
hist(mtcars$mpg)
# Load the necessary packages
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(plyr)
library(ggplot2)
# library(sentiment)
Register an application (API) at https://apps.twitter.com/
# Once done registering, look at the values of api key, secret and token
# Insert these values below:
setup_twitter_oauth("zIiEEsG2pPBH1tTBwUDy30cCI", "4Qa0uCYHJC9knJAAZsOGSbljvdAC9SqZyY9UefOnN7dVb3aKxl")
# Load the necessary packages
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(plyr)
library(ggplot2)
# library(sentiment)
Register an application (API) at https://apps.twitter.com/
# Once done registering, look at the values of api key, secret and token
# Insert these values below:
setup_twitter_oauth("zIiEEsG2pPBH1tTBwUDy30cCI", "4Qa0uCYHJC9knJAAZsOGSbljvdAC9SqZyY9UefOnN7dVb3aKxl")
# Now let us collect some tweets (2000 in our example) containing the term "BJP" from twitter (language = English,
# if you wish you can set other languages to fetch tweets in those languages in your analytics)
bjp_tweets = searchTwitter("Pioltello", n=100, lang="it")
# fetch the text of these tweets
bjp_txt = sapply(bjp_tweets, function(x) x$getText())
# Now we will prepare the above text for sentiment analysis
# First we will remove retweet entities from the stored tweets (text)
bjp_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", bjp_txt)
# Then remove all "@people"
bjp_txt = gsub("@\\w+", "", bjp_txt)
# Then remove all the punctuation
bjp_txt = gsub("[[:punct:]]", "", bjp_txt)
# Then remove numbers, we need only text for analytics
bjp_txt = gsub("[[:digit:]]", "", bjp_txt)
# the remove html links, which are not required for sentiment analysis
bjp_txt = gsub("http\\w+", "", bjp_txt)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
bjp_txt = gsub("[ \t]{2,}", "", bjp_txt)
bjp_txt = gsub("^\\s+|\\s+$", "", bjp_txt)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Since there can be some words in lower case and some in upper, we will try to eredicate this non-uniform pattern by making all the words in lower case. This makes uniform pattern.
# Let us first define a function which can handle "tolower error handling", if arises any, during converting all words in lower case.
catch.error = function(x)
{
# let us create a missing value for test purpose
y = NA
# try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
# Now we will transform all the words in lower case using catch.error function we just created above and with sapply function
bjp_txt = sapply(bjp_txt, catch.error)
# Also we will remove NAs, if any exists, from bjp_txt (the collected and refined text in analysis)
bjp_txt = bjp_txt[!is.na(bjp_txt)]
# also remove names (column headings) from the text, as we do not want them in the sentiment analysis
names(bjp_txt) = NULL
# Load the necessary packages
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(plyr)
library(ggplot2)
# library(sentiment)
Register an application (API) at https://apps.twitter.com/
# Once done registering, look at the values of api key, secret and token
# Insert these values below:
setup_twitter_oauth("zIiEEsG2pPBH1tTBwUDy30cCI", "4Qa0uCYHJC9knJAAZsOGSbljvdAC9SqZyY9UefOnN7dVb3aKxl")
# Now let us collect some tweets (2000 in our example) containing the term "BJP" from twitter (language = English,
# if you wish you can set other languages to fetch tweets in those languages in your analytics)
bjp_tweets = searchTwitter("Pioltello", n=1000, lang="it")
# fetch the text of these tweets
bjp_txt = sapply(bjp_tweets, function(x) x$getText())
# Now we will prepare the above text for sentiment analysis
# First we will remove retweet entities from the stored tweets (text)
bjp_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", bjp_txt)
# Then remove all "@people"
bjp_txt = gsub("@\\w+", "", bjp_txt)
# Then remove all the punctuation
bjp_txt = gsub("[[:punct:]]", "", bjp_txt)
# Then remove numbers, we need only text for analytics
bjp_txt = gsub("[[:digit:]]", "", bjp_txt)
# the remove html links, which are not required for sentiment analysis
bjp_txt = gsub("http\\w+", "", bjp_txt)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
bjp_txt = gsub("[ \t]{2,}", "", bjp_txt)
bjp_txt = gsub("^\\s+|\\s+$", "", bjp_txt)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Since there can be some words in lower case and some in upper, we will try to eredicate this non-uniform pattern by making all the words in lower case. This makes uniform pattern.
# Let us first define a function which can handle "tolower error handling", if arises any, during converting all words in lower case.
catch.error = function(x)
{
# let us create a missing value for test purpose
y = NA
# try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
# Now we will transform all the words in lower case using catch.error function we just created above and with sapply function
bjp_txt = sapply(bjp_txt, catch.error)
# Also we will remove NAs, if any exists, from bjp_txt (the collected and refined text in analysis)
bjp_txt = bjp_txt[!is.na(bjp_txt)]
# also remove names (column headings) from the text, as we do not want them in the sentiment analysis
names(bjp_txt) = NULL
bjp_txt
bjp_tweets
View(bjp_tweets)
str(bjp_tweets)
View(bjp_txt)
bjp_txt = sapply(bjp_tweets, function(x) x$getText())
View(bjp_txt)
library(twitteR)
?twitteR
??twitteR
library(devtools)
install.packages(devtools)
library("devtools")
install.packages("devtools")
library("devtools")
install_github("pablobarbera/Rfacebook/Rfacebook")
library(Rfacebook)
fb_oauth <- fbOAuth(app_id="1402137370112766", app_secret="d1ad158c951e0b0f30461461897b8982",extended_permissions = TRUE)
save(fb_oauth, file="fb_oauth")
load("fb_oauth")
me <- getUsers("me",token=fb_oauth)
my_likes <- getLikes(user="me", token=fb_oauth)
my_friends <- getFriends(token, simplify = FALSE)
fb_oauth <- fbOAuth(app_id="1402137370112766", app_secret="d1ad158c951e0b0f30461461897b8982",extended_permissions = TRUE)
save(fb_oauth, file="fb_oauth")
me <- getUsers("me",token=fb_oauth)
my_likes <- getLikes(user="me", token=fb_oauth)
my_friends <- getFriends(token, simplify = FALSE)
my_friends <- getFriends(token=fb_oauth, simplify = FALSE)
View(my_likes)
# Load the necessary packages
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(plyr)
library(ggplot2)
# library(sentiment)
## Register an application (API) at https://apps.twitter.com/ look at the values of api key, secret and token
setup_twitter_oauth("zIiEEsG2pPBH1tTBwUDy30cCI", "4Qa0uCYHJC9knJAAZsOGSbljvdAC9SqZyY9UefOnN7dVb3aKxl")
# Now let us collect some tweets (2000 in our example) containing the term "BJP" from twitter (language = English,
# if you wish you can set other languages to fetch tweets in those languages in your analytics)
bjp_tweets = searchTwitter("IoT", n=100, lang="en")
# fetch the text of these tweets
bjp_txt = sapply(bjp_tweets, function(x) x$getText())
# Now we will prepare the above text for sentiment analysis
# First we will remove retweet entities from the stored tweets (text)
bjp_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", bjp_txt)
# Then remove all "@people"
bjp_txt = gsub("@\\w+", "", bjp_txt)
# Then remove all the punctuation
bjp_txt = gsub("[[:punct:]]", "", bjp_txt)
# Then remove numbers, we need only text for analytics
bjp_txt = gsub("[[:digit:]]", "", bjp_txt)
# the remove html links, which are not required for sentiment analysis
bjp_txt = gsub("http\\w+", "", bjp_txt)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
bjp_txt = gsub("[ \t]{2,}", "", bjp_txt)
bjp_txt = gsub("^\\s+|\\s+$", "", bjp_txt)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Since there can be some words in lower case and some in upper, we will try to eredicate this non-uniform pattern by making all the words in lower case. This makes uniform pattern.
# Let us first define a function which can handle "tolower error handling", if arises any, during converting all words in lower case.
catch.error = function(x)
{
# let us create a missing value for test purpose
y = NA
# try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
# Now we will transform all the words in lower case using catch.error function we just created above and with sapply function
bjp_txt = sapply(bjp_txt, catch.error)
# Also we will remove NAs, if any exists, from bjp_txt (the collected and refined text in analysis)
bjp_txt = bjp_txt[!is.na(bjp_txt)]
# also remove names (column headings) from the text, as we do not want them in the sentiment analysis
names(bjp_txt) = NULL
# Now the text is fully prepared (or at least for this tutorial) and we are good to go to perform Sentiment Analysis using this text
# As a first step in this stage, let us first classify emotions
# In this tutorial we will be using Bayes’ algorithm to classify emotion categories
# for more please see help on classify_emotion (?classify_emotion) under sentiment package
#bjp_class_emo = classify_emotion(bjp_txt, algorithm="bayes", prior=1.0)
bjp_class_emo <- sentiment(bjp_txt)
# the above function returns an of bject of class data.frame with seven columns (anger, disgust, fear, joy, sadness, surprise, best_fit) and one row for each document:
# we will fetch emotion category best_fit for our analysis purposes, visitors to this tutorials are encouraged to play around with other classifications as well.
emotion = bjp_class_emo[,2]
# Replace NA’s (if any, generated during classification process) by word "unknown"
# There are chances that classification process generates NA’s. This is because, sentiment package uses an in-built dataset "emotions", which containing approximately 1500 words classified into six emotion categories: anger, disgust, fear, joy, sadness, and surprise
# If any words outside this dataset are given, the process will term the words as NA’s
emotion[is.na(emotion)] = "unknown"
# Similar to above, we will classify polarity in the text
# This process will classify the text data into four categories (pos – The absolute log likelihood of the document expressing a positive sentiment, neg – The absolute log likelihood of the document expressing a negative sentimen, pos/neg  – The ratio of absolute log likelihoods between positive and negative sentiment scores where a score of 1 indicates a neutral sentiment, less than 1 indicates a negative sentiment, and greater than 1 indicates a positive sentiment; AND best_fit – The most likely sentiment category (e.g. positive, negative, neutral) for the given text)
#bjp_class_pol = classify_polarity(bjp_txt, algorithm="bayes")
bjp_class_pol <- sentiment(bjp_txt)
View(bjp_txt)
library(sentiment)
bjp_class_emo <- sentiment(bjp_txt)
# the above function returns an of bject of class data.frame with seven columns (anger, disgust, fear, joy, sadness, surprise, best_fit) and one row for each document:
# we will fetch emotion category best_fit for our analysis purposes, visitors to this tutorials are encouraged to play around with other classifications as well.
emotion = bjp_class_emo[,2]
# Replace NA’s (if any, generated during classification process) by word "unknown"
# There are chances that classification process generates NA’s. This is because, sentiment package uses an in-built dataset "emotions", which containing approximately 1500 words classified into six emotion categories: anger, disgust, fear, joy, sadness, and surprise
# If any words outside this dataset are given, the process will term the words as NA’s
emotion[is.na(emotion)] = "unknown"
# Similar to above, we will classify polarity in the text
# This process will classify the text data into four categories (pos – The absolute log likelihood of the document expressing a positive sentiment, neg – The absolute log likelihood of the document expressing a negative sentimen, pos/neg  – The ratio of absolute log likelihoods between positive and negative sentiment scores where a score of 1 indicates a neutral sentiment, less than 1 indicates a negative sentiment, and greater than 1 indicates a positive sentiment; AND best_fit – The most likely sentiment category (e.g. positive, negative, neutral) for the given text)
#bjp_class_pol = classify_polarity(bjp_txt, algorithm="bayes")
bjp_class_pol <- sentiment(bjp_txt)
# we will fetch polarity category best_fit for our analysis purposes, and as usual, visitors to this tutorials are encouraged to play around with other classifications as well
polarity = bjp_class_pol[,2]
# Let us now create a data frame with the above results obtained and rearrange data for plotting purposes
# creating data frame using emotion category and polarity results earlier obtained
sentiment_dataframe = data.frame(text=bjp_txt, emotion=emotion, polarity=polarity, stringsAsFactors=FALSE)
# rearrange data inside the frame by sorting it
sentiment_dataframe = within(sentiment_dataframe, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
View(sentiment_dataframe)
ggplot(sentiment_dataframe, aes(x=emotion)) + geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
ggtitle(‘Sentiment Analysis of Tweets on Twitter about BJP’) +
theme(legend.position=’right’) + ylab(‘Number of Tweets’) + xlab(‘Emotion Categories’)
SA1
bjp_emos = levels(factor(sentiment_dataframe$emotion))
n_bjp_emos = length(bjp_emos)
bjp.emo.docs = rep("", n_bjp_emos)
for (i in 1:n_bjp_emos)
{
tmp = bjp_txt[emotion == bjp_emos[i]]
bjp.emo.docs[i] = paste(tmp, collapse=" ")
}
# Here is a hick. Please not that there can be words in the emotion categories which you do not want to be.
# Like earlier in this tutorial, where I asked you to remove words such as slangs etc, here also you can remove
# these words specified as stopwords.
# For exaple we take "english" as the word which we want to remove and not be present in the word cloud,
# here how we do that:
bjp.emo.docs = removeWords(bjp.emo.docs, stopwords("english"))
# Now let us create a corpus which computes and represent words on corpora (corpora are collections of documents
containing (natural language) text). For more please look at help on Corpora under "tm" package.
bjp.corpus = Corpus(VectorSource(bjp.emo.docs))
bjp.tdm = TermDocumentMatrix(bjp.corpus)
bjp.tdm = as.matrix(bjp.tdm)
colnames(bjp.tdm) = bjp_emos
# creating, comparing and plotting the words on the cloud
comparison.cloud(bjp.tdm, colors = brewer.pal(n_bjp_emos, "Dark2″),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
?frame.table
?data.frame
L3 <- LETTERS[1:3]
fac <- sample(L3, 10, replace = TRUE)
(d <- data.frame(x = 1, y = 1:10, fac = fac))
## The "same" with automatic column names:
data.frame(1, 1:10, sample(L3, 10, replace = TRUE))
is.data.frame(d)
install.packages("RMySQL")
list(1:3,1)
setwd("~/R-workspace/3-getting-and-cleaning-data")
library(httr)
# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")
# 2. To make your own application, register at at
#    https://github.com/settings/applications. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#
#    Replace your key and secret below.
myapp <- oauth_app("github",
key = "1c17bca99e9868008905",
secret = "1c9dd68d54c06bfca5deb57e6df00b2b7d26bd88")
myapp
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# 4. Use API
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
library(jsonlite)
## https://api.github.com/users/jtleek/repos
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
res <- fromJSON(content(req))
view req
View(req)
View(content(req))
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
res <- fromJSON(content(req))
View(content(req))
res <- fromJSON(toJSON(content(req)))
View(res)
res$created_at
View(res)
res$created_at[[res$name=="datasharing"]]
res$created_at[res$name=="datasharing"]
library(sqldf)
install.packages("sqldf")
library(sqldf)
acs <- read.csv("./getdata-data-ss06pid.csv", header=T, sep=",")
acs <- read.csv("./getdata-data-ss06pid.csv", header=T, sep=",")
library(sqldf)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile="./data/getdata-data-ss06pid.csv", method="curl")
dateDownloaded <- date()
acs <- read.csv("./data/getdata-data-ss06pid.csv", header=T, sep=",")
sqldf("select pwgtp1 from acs where AGEP < 50")
hurl <- "http://biostat.jhsph.edu/~jleek/contact.html"
con <- url(hurl)
htmlCode <- readLines(con)
close(con)
sapply(htmlCode[c(10, 20, 30, 100)], nchar)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileUrl, destfile="./data/getdata-wksst8110.for", method="curl")
dateDownloaded <- date()
file_name <- "./data/getdata-wksst8110.for"
df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
sum(df[, 4])
?read.fwf
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile="./data/ss06hid.csv", method="curl")
dateDownloaded <- date()
dt <- data.table(read.csv("./data/ss06hid.csv"))
agricultureLogical <- dt$ACR == 3 & dt$AGS == 6
which(agricultureLogical)[1:3]
agricultureLogical <- dt[(dt$ACR == 3 & dt$AGS == 6), ]
agricultureLogical <- (dt$ACR == 3 & dt$AGS == 6)
agricultureLogical <- (as.numeric(dt$ACR) == 3 & dt$AGS == 6)
agricultureLogical <- (as.numeric(dt$ACR) == 3 & as.numeric(dt$AGS) == 6)
View(res)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile="./data/ss06hid.csv", method="curl")
dateDownloaded <- date()
dt <- data.table(read.csv("./data/ss06hid.csv"))
library(date.table)
install.packages("ate.table")
install.packages("date.table")
library(date.table)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
#download.file(fileUrl, destfile="./data/ss06hid.csv", method="curl")
dateDownloaded <- date()
dt <- data.table(read.csv("./data/ss06hid.csv"))
dt <- date.table(read.csv("./data/ss06hid.csv"))
date.table
library(data.table)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
#download.file(fileUrl, destfile="./data/ss06hid.csv", method="curl")
dateDownloaded <- date()
dt <- data.table(read.csv("./data/ss06hid.csv"))
agricultureLogical <- dt$ACR == 3 & dt$AGS == 6)
agricultureLogical <- dt$ACR == 3 & dt$AGS == 6
which(agricultureLogical)[1:3]
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
download.file(url, "./data/jeff.jpg", mode = "wb", method = "curl")
img <- readJPEG("./data/jeff.jpg", native = TRUE)
library(jpeg)
install.packages("jpeg")
library(jpeg)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
download.file(url, "./data/jeff.jpg", mode = "wb", method = "curl")
img <- readJPEG("./data/jeff.jpg", native = TRUE)
quantile(img, probs = c(0.3, 0.8))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(url, "./data/GDP.csv", method = "curl")
dtGDP <- data.table(read.csv("./data/GDP.csv", skip = 4, nrows = 215))
dtGDP <- dtGDP[X != ""]
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP", "Long.Name", "gdp"))
# load EDSTATS_Country dataset
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(url, "./data/EDSTATS_Country.csv", method = "curl")
dtEd <- data.table(read.csv("./data/EDSTATS_Country.csv"))
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
sum(!is.na(unique(dt$rankingGDP)))
# [1] 189
dt[order(rankingGDP, decreasing = TRUE), list(CountryCode, Long.Name.x, Long.Name.y, rankingGDP, gdp)][13]
dt[, mean(rankingGDP, na.rm = TRUE), by = Income.Group]
breaks <- quantile(dt$rankingGDP, probs = seq(0, 1, 0.2), na.rm = TRUE)
dt$quantileGDP <- cut(dt$rankingGDP, breaks = breaks)
dt[Income.Group == "Lower middle income", .N, by = c("Income.Group", "quantileGDP")]
setwd("~/R-workspace")
?par
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(ggplot2)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies) + geom_smooth()
setwd("~/R-workspace/5-Reproducible-Research/RepData_PeerAssessment1")
